{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours\n",
    "\n",
    "Willkommen, in diesem Notebook lernst du, wie du einen eigenen Klassifikator implementieren kannst, der mit scikit learn kompatible ist. Als grundlage werden wir den K-Nearest Neighbour Algorithmus implementieren. Es gibt bereits eine sehr gute Implementierung durch die `KNeighboursClassifiert` Klasse aus sklearn, daher ist es nicht ratsam für \"echte\" projekte eine eigene implementierung zu verwenden. In diesem Notebook soll es darum gehen, den Aufbau eines Klassifizierers in sklearn und den KNN algorithmus besser zu verstehen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distanz- und Ähnlichkeitsfunktionen\n",
    "\n",
    "Der KNN-Algorithmus ist ein überwachtes Machine-Learning-Verfahren, das sowohl für die Klassifikation als auch für die Regression verwendet werden kann. In diesem Notebook konzentrieren wir uns auf seine Anwendung als Klassifikator.\n",
    "\n",
    "Die Grundidee des Verfahrens besteht darin, einem Datenpunkt die Klasse zuzuweisen, die in unmittelbarer Nähe des Punktes am häufigsten vorkommt. Zunächst müssen wir klären, was mit **Umgebung** gemeint ist.\n",
    "\n",
    "Wie bei den meisten Machine-Learning-Verfahren müssen wir unsere Datenpunkte als Vektoren in einem n-dimensionalen Vektorraum darstellen können. Das bedeutet im Grunde genommen, dass ein Datenpunkt $X$ eine Sequenz von Zahlen $x_1, x_2, ..., x_n$ darstellt. Die Länge $n$ der Sequenz $X$ ist dabei immer fest.\n",
    "\n",
    "Um die **Umgebung** eines Datenpunkts im KNN-Algorithmus zu definieren, benötigen wir geeignete Distanz- oder Ähnlichkeitsmetriken. Diese Metriken helfen uns, die Nähe zwischen Datenpunkten zu quantifizieren. Im Folgenden betrachten wir drei gängige Metriken: die euklidische Distanz, die Manhattan-Distanz und die Cosinus-Ähnlichkeit.\n",
    "\n",
    "### Euklidische Distanz\n",
    "\n",
    "Die euklidische Distanz ist wahrscheinlich die am häufigsten verwendete Metrik und berechnet die \"Luftlinie\" zwischen zwei Punkten in einem n-dimensionalen Raum. Sie wird durch die folgende Formel beschrieben:\n",
    "\n",
    "$$\n",
    "d_{euclid}(X, Y) = \\sqrt{\\sum_{i=1}^{n} (x_i - y_i)^2}\n",
    "$$\n",
    "\n",
    "Hierbei sind $X$ und $Y$ zwei Datenpunkte, dargestellt als Vektoren $(x_1, x_2, ..., x_n)$ und $(y_1, y_2, ..., y_n)$. Die euklidische Distanz misst die direkte Strecke zwischen den Punkten und ist besonders nützlich, wenn die Datenpunkte kontinuierliche Werte haben.\n",
    "\n",
    "![Euklidische Distanz](images/euclidean_distance.jpg)\n",
    "\n",
    "### Manhattan-Distanz\n",
    "\n",
    "Die Manhattan-Distanz, auch bekannt als L1-Distanz oder Taxicab-Distanz, summiert die absoluten Differenzen der einzelnen Komponenten. Sie wird durch die folgende Formel beschrieben:\n",
    "\n",
    "$$\n",
    "d_{manhattan}(X, Y) = \\sum_{i=1}^{n} |x_i - y_i|\n",
    "$$\n",
    "\n",
    "Diese Metrik misst die Strecke entlang der Achsen des Vektorraums, ähnlich wie ein Taxi in einer Stadt mit einem rechtwinkligen Straßennetz fahren würde. Die Manhattan-Distanz kann besonders nützlich sein, wenn die Daten viele Ausreißer enthalten oder diskrete Werte angenommen werden.\n",
    "\n",
    "![Manhattan-Distanz](images/manhattan_distance.jpg)\n",
    "\n",
    "### Cosinus-Ähnlichkeit\n",
    "\n",
    "Die Cosinus-Ähnlichkeit unterscheidet sich von den vorherigen Metriken, da sie den Winkel zwischen zwei Vektoren im n-dimensionalen Raum misst, anstatt die tatsächliche Distanz. Sie wird durch die folgende Formel beschrieben:\n",
    "\n",
    "$$\n",
    "\\text{cos}(X, Y) = \\frac{X \\cdot Y}{\\|X\\| \\|Y\\|} = \\frac{\\sum_{i=1}^{n} x_i y_i}{\\sqrt{\\sum_{i=1}^{n} x_i^2} \\sqrt{\\sum_{i=1}^{n} y_i^2}}\n",
    "$$\n",
    "\n",
    "Hierbei ist $X \\cdot Y$ das Skalarprodukt der Vektoren $X$ und $Y$, und $\\|X\\|$ sowie $\\|Y\\|$ sind die euklidischen Normen der Vektoren $X$ und $Y$.\n",
    "\n",
    "Die Cosinus-Ähnlichkeit nimmt Werte zwischen -1 und 1 an, wobei 1 bedeutet, dass die Vektoren in die gleiche Richtung zeigen, 0 bedeutet, dass sie orthogonal sind, und -1 bedeutet, dass sie in entgegengesetzte Richtungen zeigen. Diese Metrik ist besonders nützlich, wenn es darum geht, die Richtung von Vektoren zu vergleichen, unabhängig von ihrer Größe.\n",
    "\n",
    "![Cosinus-Ähnlichkeit](images/cosine_similarity.jpg)\n",
    "\n",
    "### Anwendung im KNN-Algorithmus\n",
    "\n",
    "Im KNN-Algorithmus bestimmen wir die \"nächsten Nachbarn\" eines Datenpunkts basierend auf diesen Distanz- oder Ähnlichkeitsmetriken. Die Wahl der Metrik kann die Leistung des Algorithmus erheblich beeinflussen und hängt oft von den spezifischen Eigenschaften der Daten ab.\n",
    "\n",
    "Für die Implementierung in diesem Notebook werden wir uns zunächst auf die euklidische Distanz konzentrieren, da sie eine intuitive und weit verbreitete Methode ist, um die Nähe zwischen Datenpunkten zu messen. Sie ermöglicht klare und direkte Vergleiche zwischen den Punkten, was besonders hilfreich ist, um die Funktionsweise des KNN-Algorithmus zu verstehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginne damit eine Funktion zur berechnung der Euklidischen Distanz zu schreiben. Die Funktion bekommt als eingabe zwei n-dimensionale Punkte in Form von numpy arrays und soll den Abstand nach Euclid als Zahl zurückgebe.\n",
    "Hierfür sind die numpy Funktionen `np.sqrt` und `np.sum` zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n",
    "    return np.sqrt(np.sum((a - b) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In der Folgenden Zelle sind einige Testfälle mit denen du die Korrektheit deiner Implementierung überprüfen kannst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "test"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Tests erfolgreich bestanden!\n"
     ]
    }
   ],
   "source": [
    "# Testfälle\n",
    "\n",
    "# Test 1: Identische Punkte\n",
    "a = np.array([1, 2])\n",
    "b = np.array([1, 2])\n",
    "assert euclidean_distance(a, b) == 0.0, \"Fehler in Test 1: Identische Punkte\"\n",
    "\n",
    "# Test 2: Einfache Punkte\n",
    "a = np.array([0, 0])\n",
    "b = np.array([3, 4])\n",
    "assert euclidean_distance(a, b) == 5.0, \"Fehler in Test 2: Einfache Punkte (3-4-5 Dreieck)\"\n",
    "\n",
    "# Test 3: Punkte mit negativen Werten\n",
    "a = np.array([-1, -2])\n",
    "b = np.array([1, 2])\n",
    "assert np.isclose(euclidean_distance(a, b), 4.4721, atol=1e-4), \"Fehler in Test 3: Punkte mit negativen Werten\"\n",
    "\n",
    "# Test 4: Höherdimensionale Punkte\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "assert np.isclose(euclidean_distance(a, b), 5.1962, atol=1e-4), \"Fehler in Test 4: Höherdimensionale Punkte\"\n",
    "\n",
    "# Test 5: Höherdimensionale Punkte mit negativen Werten\n",
    "# Test 5: Höherdimensionale Punkte mit negativen Werten\n",
    "a = np.array([-1, -2, -3])\n",
    "b = np.array([4, 5, 6])\n",
    "expected_distance = np.sqrt(155)\n",
    "assert np.isclose(euclidean_distance(a, b), expected_distance, atol=1e-4), \"Fehler in Test 5: Höherdimensionale Punkte mit negativen Werten\"\n",
    "\n",
    "print(\"Alle Tests erfolgreich bestanden!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut. Nun wollen wir einen Schritt weiter gehen und auf basis unserer Abstandsmetrik die k-nächsten Nachbarn eines bestimmten Punktes aus einer Menge weiterer Punkte ermitteln.\n",
    "Verfollständige die funktion `find_k_neighbours`. Die Funktion bekommt einen Punkt `point` sowie eine Sequenz von weiteren Punkten `X` sowie den Parameter `k`.\n",
    "Zunächst soll der Abstand von `point` zu jedem weiteren Punkt in `X` ermittelt werden. Auf basis dieser Distanzen können dann die inidzes derjedingen Punkte gefunden werden, welche dem Punk `point` am nächsten liegen.\n",
    "\n",
    "Für diese Aufgabe kannst du die zuvor geschriebene `euclidean_distance` Funktion verwenden. Zudem hilft dir die Funktion `np.argsort` die indizes nach der distanz zu sortieren.\n",
    "\n",
    "Die Funktion `check_array` aus `sklearn.utils` ist Hilfreich um sicherzugehen, dass deine Funktion sowohl mit numpy arrays, als auch mit Pandas datenstrukturen funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_array\n",
    "\n",
    "def find_k_neighbours(point, X, k=5):\n",
    "    \"\"\"\n",
    "    Findet die k nächsten Nachbarn eines gegebenen Punktes in einem Datensatz.\n",
    "\n",
    "    Parameter:\n",
    "    point (array-like): Ein eindimensionaler Vektor, der den Punkt darstellt, für den die nächsten Nachbarn gefunden werden sollen.\n",
    "    X (array-like): Ein zweidimensionaler Datensatz, in dem die nächsten Nachbarn gesucht werden sollen. Jede Zeile repräsentiert einen Punkt im Raum.\n",
    "    k (int, optional): Die Anzahl der nächsten Nachbarn, die gefunden werden sollen. Standardmäßig ist k=5.\n",
    "\n",
    "    Rückgabe:\n",
    "    indices (numpy.ndarray): Ein Array mit den Indizes der k nächsten Nachbarn im Datensatz X.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    X = check_array(X)\n",
    "    point = check_array(point, ensure_2d=False)\n",
    "    \n",
    "    # Berechnen Sie die euklidische Distanz zu jedem Punkt im Trainingssatz\n",
    "    distances = [euclidean_distance(point, x) for x in X]\n",
    "\n",
    "    # Finden Sie die Indizes der k nächsten Nachbarn\n",
    "    indices = np.argsort(distances)[:k]\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Tests erfolgreich bestanden!\n"
     ]
    }
   ],
   "source": [
    "# Testfall 1: Einfacher Testfall mit zwei Punkten\n",
    "X = np.array([[1, 2], [3, 4]])\n",
    "point = np.array([2, 3])\n",
    "expected_result = np.array([0, 1])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=2), expected_result)\n",
    "\n",
    "# Testfall 2: Testfall mit drei Punkten, wobei einer der Punkte der gleiche wie der gegebene Punkt ist\n",
    "X = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "point = np.array([2, 3])\n",
    "expected_result = np.array([1, 0, 2])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=3), expected_result)\n",
    "\n",
    "# Testfall 3: Testfall mit mehreren Punkten und k=1\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "point = np.array([3, 4])\n",
    "expected_result = np.array([2])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=1), expected_result)\n",
    "\n",
    "# Testfall 4: Testfall mit mehreren Punkten in höheren Dimensionen\n",
    "X = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "point = np.array([5, 6, 7])\n",
    "expected_result = np.array([1, 2, 0])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=3), expected_result)\n",
    "\n",
    "# Testfall 5: Testfall mit einem Pandas DataFrame\n",
    "X = pd.DataFrame({\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [2, 3, 4, 5, 6],\n",
    "    'z': [3, 4, 5, 6, 7]\n",
    "})\n",
    "point = np.array([3, 4, 5])\n",
    "expected_result = np.array([2, 1, 3, 0, 4])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=5), expected_result)\n",
    "\n",
    "# Testfall 6: Testfall mit negativen Koordinaten\n",
    "X = np.array([[-1, -2], [-3, -4], [-5, -6]])\n",
    "point = np.array([-4, -5])\n",
    "expected_result = np.array([1, 2, 0])\n",
    "assert np.array_equal(find_k_neighbours(point, X, k=3), expected_result)\n",
    "\n",
    "print(\"Alle Tests erfolgreich bestanden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sehr gut. Lass uns nun die Funktion an einem realen Datensatz testen. Hierfür nehmen wir uns den Iris Datensatz zu Hilfe, welcher 150 Datenpunkte mit 4 Features und einer Targetklasse aus 3 möglichen Ausprägungen beinnhaltet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "145    2\n",
       "146    2\n",
       "147    2\n",
       "148    2\n",
       "149    2\n",
       "Name: target, Length: 150, dtype: int32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris(as_frame=True)\n",
    "X, y = iris.data, iris.target\n",
    "display(X)\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmen wir beispielsweise den Punkt mit dem Index 45. Wenn wir uns für k=1 entscheiden und eine Punkt wählen der bereits in X enthalten ist, dann ist klar, dass genau dieser Punkt der nächstgelegene Nachbar ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_k_neighbours(X.iloc[45, :], X, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erweitern wir den Suchraum auf k=10 bekommen wir die Indizes der 10 nächstgelegenen Punkte. Diese Indizes können wir dann benutzen um die Target werte dieser Punkte aus y zu bestimmen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index      | class\n",
      "        56 | 1    \n",
      "        51 | 1    \n",
      "        85 | 1    \n",
      "        91 | 1    \n",
      "       127 | 2    \n",
      "        86 | 1    \n",
      "        70 | 1    \n",
      "       138 | 2    \n",
      "        63 | 1    \n",
      "        78 | 1    \n"
     ]
    }
   ],
   "source": [
    "point_idx = 56\n",
    "k=10\n",
    "neighbours_indizes = find_k_neighbours(X.iloc[point_idx, :], X, k=k)\n",
    "neighbours_classes = y[neighbours_indizes]\n",
    "\n",
    "print(f\"{'index':<10} | {'class':<5}\")\n",
    "for neighbour_idx, neighbour_class in zip(neighbours_indizes, neighbours_classes):\n",
    "    # print(neighbour_idx, neighbour_class)\n",
    "    print(f\"{neighbour_idx:>10} | {neighbour_class:<5}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Wir sehen, dass der Punkt mit dem Index 56 zur Klasse 1 gehört und dass sich in seiner unmittelbaren Nachbarschaft überwiegend Punkte aus eben dieser Klasse sowie ein paar Punkte der Klasse 2 befinden. Wir wollen nun die häufigste dieser Klassen auswählen.\n",
    "\n",
    "Dafür können wir die Funktion `np.bincount` nutzen. Diese zählt die Häufigkeit jedes Wertes eines Arrays. Die Häufigkeiten werden dem Index des Arrays zugeordnet, dessen Wert sie darstellen. Das heißt, wenn der Wert 3 fünfmal vorkommt, gibt `bincount` ein Array zurück, an dessen dritter Stelle eine Fünf steht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount([0, 0, 0, 1, 1, 3,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Wert 0 kommt drei mal vor, daher steht in der Ausgabe an der index position 0 der wert 3.\n",
    "\n",
    "Zähle nun mit bincount die häufigkeit der zuvor ermittelten Klassen und speichere sie als variable `counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 2], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zählen Sie die Anzahl der Instanzen jeder Klasse\n",
    "counts = np.bincount(neighbours_classes) \n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit der Funktion `np.argmax` kannst du nun den Index mit der Größten Zahl ermitteln. Dieser Entspricht dann der Klasse, die am Häufigsten in der Nachbarschaft anzutreffen ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementierung als Klasse\n",
    "\n",
    "Sehr gut. Wir haben nun alle wichtigen Schritte besprochen, um den K-Nearest Neighbour Klassifizierer zu verstehen. Als nächstes wollen wir eine Klasse schreiben, um den Algorithmus wie einen `sklearn`-Classifier nutzen zu können.\n",
    "\n",
    "### Grundaufbau\n",
    "\n",
    "Dafür müssen wir die Grundstruktur von `sklearn`-Classifiern übernehmen. Ein Classifier in `sklearn` besitzt mindestens 3 Methoden:\n",
    "\n",
    "- `__init__`: Diese Methode wird verwendet, um ein Objekt der Klasse zu instanzieren. Ihr wird der Parameter `k` übergeben, welcher als Attribut des Objekts abgespeichert wird.\n",
    "\n",
    "- `fit`: Diese Methode wird für das Trainieren eines Classifiers verwendet. Sie dient dazu, unser Objekt mit den Trainingsdaten vertraut zu machen. Diese Trainingsdaten bilden letztlich die Nachbarschaft, in der nach ähnlichen Punkten gesucht wird. Da KNN ein Offline-Verfahren ist, reicht es, wenn wir uns die Trainingsdaten zunächst als Attribute des Objekts speichern. Allerdings ist zu beachten, dass diese Attribute mit einem `_` am Ende des Variablennamens enden.\n",
    "\n",
    "- `predict`: Jeder Klassifikator implementiert die `predict`-Methode, um einen neuen Datenpunkt oder eine Reihe von Punkten zu klassifizieren. Diese Methode darf erst aufgerufen werden, wenn die `fit`-Methode bereits aufgerufen wurde. Um dies zu überprüfen, können wir die `check_is_fitted`-Funktion aus `sklearn.utils` verwenden. Diese überprüft, ob das Objekt Attribute mit einem `_` am Ende des Variablennamens besitzt. Danach können wir die bereits gelernten Techniken anwenden, um zu jedem Punkt eine Klasse aus den nächstgelegenen Nachbarn zu ermitteln. Hierfür bietet es sich an, die Funktion `euclidean_distance` als private Methode zu implementieren. Das hat den Vorteil, dass wir die Klasse leicht um weitere Metriken erweitern können.\n",
    "\n",
    "### Einbeziehung von Basisklassen\n",
    "\n",
    "Neben den von uns diskutierten Methoden `__init__`, `fit` und `predict` ist es auch wichtig, dass wir unsere K-Nearest Neighbour-Klasse von den Basisklassen `BaseEstimator` und `ClassifierMixin` aus dem `sklearn.base`-Modul ableiten.\n",
    "\n",
    "Die Klasse `BaseEstimator` gibt unseren eigenen Estimator-Klassen nützliche Methoden wie `get_params` und `set_params`, die uns erlauben, die Parameter unserer Klassifizierer im `sklearn`-Stil zu verwalten. Dies ist besonders nützlich, wenn wir unser Modell mit verschiedenen Parametern optimieren möchten, da wir die Parameter einfach als Argumente an `set_params` weitergeben können. Dies macht es beispielsweise möglich die Parameter eines Modells durch `GridSearch` oder `RandomizedSearch` optimieren zu lassen.\n",
    "\n",
    "Die Klasse `ClassifierMixin` ergänzt unsere Klasse mit der `score`-Methode, die ein einfaches Mittel zur Bewertung der Leistung des Klassifikators auf Testdaten bietet. Sie berechnet standardmäßig die Genauigkeit des Klassifikators, kann aber mit anderen Metriken überschrieben werden, wenn dies für unseren Anwendungsfall erforderlich ist.\n",
    "\n",
    "Um von diesen Klassen zu erben, fügen wir einfach `BaseEstimator` und `ClassifierMixin` in die Klassendefinition ein.\n",
    "\n",
    "\n",
    "Durch die Verwendung dieser Basisklassen stellen wir sicher, dass unser K-Nearest Neighbor-Klassifizierer nahtlos in die `sklearn`-Pipeline und verwandte Funktionen integriert werden kann.\n",
    "\n",
    "Implementiere nun die Klasse mit den entsprechenden Funktionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "class SimpleKNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k=3):\n",
    "        ...\n",
    "\n",
    "    def _euclidean_distance(self, a, b):\n",
    "        ...\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        ...\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        ...\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "import numpy as np\n",
    "\n",
    "class SimpleKNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def _euclidean_distance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Überprüfen, dass X und y die richtige Form haben\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.classes_, self.y_ = np.unique(y, return_inverse=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Überprüfen, ob fit aufgerufen wurde\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Eingabe validieren\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Vorhersagen für jeden Punkt\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            # Berechnen Sie die euklidische Distanz zu jedem Punkt im Trainingssatz\n",
    "            distances = [self._euclidean_distance(x, x_train) for x_train in self.X_]\n",
    "\n",
    "            # Finden Sie die Indizes der k nächsten Nachbarn\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Zählen Sie die Anzahl der Instanzen jeder Klasse\n",
    "            counts = np.bincount(self.y_[indices], minlength=len(self.classes_))\n",
    "\n",
    "            # Die Klasse mit den meisten Instanzen ist die Vorhersage\n",
    "            prediction = self.classes_[np.argmax(counts)]\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um unsere Implementierung fachgerecht zu testen müssen wir unsere Datanbasis in ein test- und ein train-Set aufteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y.to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 4)\n",
      "(112, 1)\n",
      "(38, 4)\n",
      "(38, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir unser Modell mit dem traingsdatensatz fitten und danach mit dem test datensatz die Performance unseres Modells testen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        11\n",
      "           1       0.92      1.00      0.96        11\n",
      "           2       1.00      0.94      0.97        16\n",
      "\n",
      "    accuracy                           0.97        38\n",
      "   macro avg       0.97      0.98      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nils Schillmann\\.conda\\envs\\stackfuel\\Lib\\site-packages\\sklearn\\utils\\validation.py:1184: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "sknn = SimpleKNNClassifier(k=5)\n",
    "sknn.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(classification_report(y_test, sknn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Herzlichen Glückwunsch! Du hast erfolgreich einen eigenen Klassifikator erstellt, der nahtlos mit dem Ökosystem von Scikit-learn kompatibel ist.\n",
    "\n",
    "Falls du nun Interesse daran hast, deine Implementierung zu optimieren, gibt es einige Möglichkeiten:\n",
    "\n",
    "- Du könntest die Klasse in eine `.py`-Datei auslagern. Auf diese Weise kannst du die Klasse in beliebigen Skripten und Notebooks laden.\n",
    "- Eine weitere Möglichkeit wäre, die Implementierung um zusätzliche Metriken wie die Manhattan-Distanz oder die Kosinus-Ähnlichkeit zu erweitern.\n",
    "- Du könntest auch die Leistung verbessern. Unsere aktuelle Implementierung ist relativ einfach und könnte bei großen Datenmengen recht lange dauern. Die Implementierung kann weiter optimiert werden, damit beim Vorhersagen nicht zwingend der gesamte Trainingssatz durchsucht werden muss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "import numpy as np\n",
    "\n",
    "class SimpleKNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, k=3, distance_func='euclidean'):\n",
    "        self.k = k\n",
    "        self.distance_func = distance_func\n",
    "\n",
    "    def _euclidean_distance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def _manhattan_distance(self, a, b):\n",
    "        return np.sum(np.abs(a - b))\n",
    "\n",
    "    def _cosine_similarity(self, a, b):\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Überprüfen, dass X und y die richtige Form haben\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.classes_, self.y_ = np.unique(y, return_inverse=True)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Überprüfen, ob fit aufgerufen wurde\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Eingabe validieren\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Wählen Sie die Distanzfunktion\n",
    "        if self.distance_func == 'euclidean':\n",
    "            distance_func = self._euclidean_distance\n",
    "        elif self.distance_func == 'manhattan':\n",
    "            distance_func = self._manhattan_distance\n",
    "        elif self.distance_func == 'cosine':\n",
    "            distance_func = self._cosine_similarity\n",
    "        else:\n",
    "            raise ValueError(\"Unbekannte Distanzfunktion: {}\".format(self.distance_func))\n",
    "\n",
    "        # Vorhersagen für jeden Punkt\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            # Berechnen Sie die Distanz zu jedem Punkt im Trainingssatz\n",
    "            distances = [distance_func(x, x_train) for x_train in self.X_]\n",
    "\n",
    "            # Finden Sie die Indizes der k nächsten Nachbarn\n",
    "            indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Zählen Sie die Anzahl der Instanzen jeder Klasse\n",
    "            counts = np.bincount(self.y_[indices], minlength=len(self.classes_))\n",
    "\n",
    "            # Die Klasse mit den meisten Instanzen ist die Vorhersage\n",
    "            prediction = self.classes_[np.argmax(counts)]\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackfuel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
